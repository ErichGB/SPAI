{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cca16d5",
   "metadata": {},
   "source": [
    "Integrantes: \n",
    "Erich Gonzalez\n",
    "Daniel Gutierrez San José\n",
    "Georgelys Marcano\n",
    "\n",
    "Enunciado:\n",
    "Calcular mediante paralelizacion con Spark la media y varianza del dataset data_ok.csv utilizando exclusivamente funciones basicas map/reduce (textFile, reduce, reduceByKey, map, flatmap, filter, count).\n",
    "\n",
    "Calcular inicialmente para una sola columna y mas tarde para todas las columnas del dataset.\n",
    "\n",
    "Verificar que la solucion propuesta es correcta con las funciones rdd.mean() y rdd.stdev().\n",
    "\n",
    "Se deben realizar 3 versiones:\n",
    "\n",
    "a1: Calcula la media y varianza para la columna 2\n",
    "\n",
    "a2: utilizando las operaciones de vectorizacion de python y arrays de numpy, utilizar la misma estructura de codigo de la version 1 para calcular las medias y varianzas de todas las columnas\n",
    "\n",
    "a3: Transforma cada celda del dataset en un elemento (j,v),  donde \"j\" es la columna de la celda y \"v\" es el valor de la celda del rdd.\n",
    "Resuelve el problema con esta nueva estructura del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cde55d",
   "metadata": {},
   "source": [
    "Versióna a1. Cáclulo de la varianza y la media para la columna 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2991e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf6f063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 16:18:13 WARN Utils: Your hostname, DESKTOP-KLPDI5O resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/20 16:18:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/georgelysm/miniconda3/envs/py37/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 16:18:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/20 16:18:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local[*]\",\"data_okCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d19009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La media de los valores de la columna2 son: 8544.5638\n",
      "La varianza es:  3168373338.3683057\n",
      "La desviación estandar es: 56288.3055204925\n"
     ]
    }
   ],
   "source": [
    "path = \"./data_ok.csv\"\n",
    "\n",
    "rdd = sc.textFile(path)\n",
    "\n",
    "# Se divide en rows eñ dataset\n",
    "pares = rdd.map(lambda x: float(x.split()[1]))\n",
    "\n",
    "#Se obtiene la suma de los valores de la columna2 para la media\n",
    "suma_columna2 = pares.reduce(lambda x,y : x+y)\n",
    "\n",
    "#Se obtiene el total de elementos con count\n",
    "total_columna2 = pares.count()\n",
    "\n",
    "#media de los datos de la columna2\n",
    "media = suma_columna2/total_columna2 \n",
    "print(\"La media de los valores de la columna2 son:\", media)\n",
    "\n",
    "#se obtiene la diferencia de cada elemento menos la media\n",
    "dif_cuadrados = pares.map(lambda x: float(x - media)**2)\n",
    "\n",
    "#Se suma los resultados obtenidos del map anterior\n",
    "sum_dif_cuadrados = dif_cuadrados.reduce(lambda x,y: x+y)\n",
    "\n",
    "#Se calcula la varianza\n",
    "varianza = sum_dif_cuadrados/total_columna2\n",
    "print(\"La varianza es: \", varianza)\n",
    "\n",
    "#Se calcula la desviación estandar\n",
    "desv = varianza**(1/2)\n",
    "print(\"La desviación estándar es:\" , desv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
