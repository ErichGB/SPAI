{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a21aa9",
   "metadata": {},
   "source": [
    "**Integrantes: Erich Gonzalez, Daniel Gutierrez San José y Georgelys Marcano**\n",
    "\n",
    "Enunciado: \n",
    "\n",
    "    - Calcular mediante paralelizacion con Spark la media y varianza del dataset data_ok.csv utilizando exclusivamente funciones basicas map/reduce (textFile, reduce, reduceByKey, map, flatmap, filter, count).\n",
    "\n",
    "    - Calcular inicialmente para una sola columna y mas tarde para todas las columnas del dataset.\n",
    "\n",
    "    - Verificar que la solucion propuesta es correcta con las funciones rdd.mean() y rdd.stdev().\n",
    "\n",
    "Se deben realizar 3 versiones:\n",
    "\n",
    "    a1: Calcula la media y varianza para la columna 4\n",
    "\n",
    "    a2: utilizando las operaciones de vectorizacion de python y arrays de numpy, utilizar la misma estructura de codigo de la version 1 para calcular las medias y varianzas de todas las columnas\n",
    "\n",
    "    a3: Transforma cada celda del dataset en un elemento (j,v), donde \"j\" es la columna de la celda y \"v\" es el valor de la celda del rdd. Resuelve el problema con esta nueva estructura del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cde55d",
   "metadata": {},
   "source": [
    "### Versión a1: Calcula la media y varianza para la columna 4\n",
    "\n",
    "Para calcular la media y la varianza de los datos correspondientes a la columna 4, se hizo uso de las siguientes fórmulas:\n",
    "    varianza=(∑𝑥2𝑖𝑛)−(∑𝑥𝑖𝑛)2, lo que es equivalente a:\n",
    "                                    1𝑛∑𝑖=1𝑛(𝑥𝑖−𝜇)2\n",
    "    La media aritmética:\n",
    "                                    1𝑛∑𝑖=1𝑛(𝑥𝑖)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2991e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6f063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\",\"data_okCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d19009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#se debe ajustar la ruta del archivo para que no provoque error al ejecutar la celda\n",
    "path = \"./data_ok.csv\" \n",
    "\n",
    "rdd = sc.textFile(path)\n",
    "\n",
    "#Se divide el dataset en rows\n",
    "rows = rdd.map(lambda x: float(x.split()[3]))\n",
    "\n",
    "#Se suma los valores de la columna 4 mediante la acción reduce\n",
    "sum_column4 = rows.reduce(lambda x,y : x+y)\n",
    "\n",
    "#Se obtiene el total de elementos con count\n",
    "total_column4 = rows.count()\n",
    "\n",
    "#Cálculo de la media de los datos de la columna4\n",
    "means = sum_column4/total_column4 \n",
    "print(\"La media de los valores de la columna 4 son:\", means)\n",
    "\n",
    "#se obtiene la diferencia de cada elemento menos la media\n",
    "dif_squart = rows.map(lambda x: float(x - means)**2)\n",
    "\n",
    "#Se suma los resultados obtenidos del map anterior\n",
    "sum_dif_quarts = dif_squart.reduce(lambda x,y: x+y)\n",
    "\n",
    "#Se calcula la varianza\n",
    "variance = sum_dif_quarts/total_column4\n",
    "print(\"La varianza es: \", variance)\n",
    "\n",
    "#Se calcula la desviación estandar\n",
    "desv = variance**(1/2)\n",
    "print(\"La desviación estándar es:\" , desv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
