{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d623e788",
   "metadata": {},
   "source": [
    "**Integrantes: Erich Gonzalez, Daniel Gutierrez San José y Georgelys Marcano**\n",
    "\n",
    "Enunciado: \n",
    "\n",
    "    - Calcular mediante paralelizacion con Spark la media y varianza del dataset data_ok.csv utilizando exclusivamente funciones basicas map/reduce (textFile, reduce, reduceByKey, map, flatmap, filter, count).\n",
    "\n",
    "    - Calcular inicialmente para una sola columna y mas tarde para todas las columnas del dataset.\n",
    "\n",
    "    - Verificar que la solucion propuesta es correcta con las funciones rdd.mean() y rdd.stdev().\n",
    "\n",
    "Se deben realizar 3 versiones:\n",
    "\n",
    "    a1: Calcula la media y varianza para la columna 4\n",
    "\n",
    "    a2: utilizando las operaciones de vectorizacion de python y arrays de numpy, utilizar la misma estructura de codigo de la version 1 para calcular las medias y varianzas de todas las columnas\n",
    "\n",
    "    a3: Transforma cada celda del dataset en un elemento (j,v), donde \"j\" es la columna de la celda y \"v\" es el valor de la celda del rdd. Resuelve el problema con esta nueva estructura del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5bbaba",
   "metadata": {},
   "source": [
    "### Versión a2: utilizando las operaciones de vectorizacion de python y arrays de numpy, utilizar la misma estructura de codigo de la version 1 para calcular las medias y varianzas de todas las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83fce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352bd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\",\"v.a2-media-varianza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4118fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ruta del csv\n",
    "path = \"./data_ok.csv\" \n",
    "\n",
    "#se ingresa la ruta en file del sc (rdd)\n",
    "rdd = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee198232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd1 (se toman las celdas de cada row y obtiene un dataset de arrays donde cada valor es un row)\n",
    "rdd1 = rdd.map(lambda linea:linea.split())\n",
    "\n",
    "#rdd2 (se convierte en un array, donde cada elemento es un array, que luego se covierte a float)\n",
    "rdd2 = rdd1.map(lambda array: np.array(array).astype(float))\n",
    "\n",
    "#se obtiene la cantidad de elementos del array (5000)\n",
    "num_elements = rdd2.count() \n",
    "\n",
    "#rdd3 se obtiene la suma de los elementos de cada fila del array\n",
    "sum_elements = rdd2.reduce(lambda x,y:x+y)\n",
    "\n",
    "#Se obtiene la media aritmética de los elementos de los arrays del dataset\n",
    "means = sum_elements/num_elements\n",
    "\n",
    "#Se obtiene la diferencia de cuadrados\n",
    "dif_quarts = rdd2.map(lambda x: np.power((x-means),2))\n",
    "\n",
    "#Se obtiene la desviación estándar\n",
    "sum_dif_elements = dif_quarts.reduce(lambda x,y: x+y)\n",
    "\n",
    "#Se obtiene la varianza de los elementos\n",
    "variance = sum_dif_elements/num_elements\n",
    "\n",
    "#se obtiene la desviación estándar\n",
    "desvstd = np.power(variance, 1/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
